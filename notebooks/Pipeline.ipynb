{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44edf073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from argparse import ArgumentParser\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from src.utils.DataLoader import HidaDataLoader\n",
    "import pandas as pd\n",
    "import scipy.special as sc\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "from transformers import ViTFeatureExtractor\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f38901",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(size=(900,900)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4fb8c0",
   "metadata": {},
   "source": [
    "# get predictions from ResNet image classifier1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf06fac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 44/44 [06:42<00:00,  9.15s/it]\n"
     ]
    }
   ],
   "source": [
    "dl = HidaDataLoader(num_workers=0, batch_size=8, data_path=\"../data\", transform=transform)\n",
    "dl.train_split = 0.6\n",
    "dl.setup()\n",
    "\n",
    "ONNX_FILE = \"C:/Users/Tobias/PycharmProjects/HIDA_LFL/logs/checkpoints/HIDA/model_193.onnx\"\n",
    "options = ort.SessionOptions()\n",
    "options.inter_op_num_threads = 12\n",
    "options.intra_op_num_threads = 12\n",
    "\n",
    "ort_sess = ort.InferenceSession(ONNX_FILE, sess_options=options)\n",
    "ort_sess.get_inputs()[0].name\n",
    "\n",
    "valid_dataloader = dl.val_dataloader()\n",
    "\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "output_name = ort_sess.get_outputs()[0].name\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "counter = 0\n",
    "tmpfile = \"tmpfile1.csv\"\n",
    "if os.path.isfile(tmpfile):\n",
    "    resnet_df = pd.read_csv(tmpfile)\n",
    "else:\n",
    "    for batch in tqdm(valid_dataloader):\n",
    "        model_input, label, (label_name, image_name) = batch\n",
    "\n",
    "        outputs_single = sc.expit(ort_sess.run([output_name], {input_name: model_input.cpu().numpy()})[0]).T[0]\n",
    "        predictions += list(outputs_single)\n",
    "        targets += list(image_name)\n",
    "        counter += 1\n",
    "\n",
    "    resnset_results = dict(predictions_resnet=predictions, images=targets)\n",
    "    resnet_df = pd.DataFrame(resnset_results)\n",
    "    resnet_df.to_csv(tmpfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206c1671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = HidaDataLoader(num_workers=0, batch_size=2, data_path=\"../data\", transform=transform)\n",
    "# dl.train_split = 0.6\n",
    "# dl.setup()\n",
    "\n",
    "# ONNX_FILE = \"C:/Users/Tobias/Downloads/model_9.onnx\"\n",
    "# options = ort.SessionOptions()\n",
    "# options.inter_op_num_threads = 12\n",
    "# options.intra_op_num_threads = 12\n",
    "\n",
    "# ort_sess2 = ort.InferenceSession(ONNX_FILE, sess_options=options)\n",
    "# ort_sess2.get_inputs()[0].name\n",
    "\n",
    "# valid_dataloader = dl.val_dataloader()\n",
    "\n",
    "# input_name = ort_sess2.get_inputs()[0].name\n",
    "# output_name = ort_sess2.get_outputs()[0].name\n",
    "\n",
    "# predictions = []\n",
    "# targets = []\n",
    "\n",
    "\n",
    "# feature_extractor = ViTFeatureExtractor(do_resize=False, do_normalize=False)\n",
    "\n",
    "# counter = 0\n",
    "# for batch in tqdm(valid_dataloader):\n",
    "#     model_input, label, (label_name, image_name) = batch\n",
    "#     features = feature_extractor(model_input, return_tensors=\"pt\")\n",
    "#     outputs_single = sc.expit(ort_sess2.run([output_name], {input_name: features[\"pixel_values\"][0]})[0]).T[0]\n",
    "#     predictions += list(outputs_single)\n",
    "#     targets += list(image_name)\n",
    "#     counter += 1\n",
    "\n",
    "# vision_results = dict(predictions_resnet=predictions, images=targets)\n",
    "# vision_df = pd.DataFrame(vision_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd85b",
   "metadata": {},
   "source": [
    "# impute datasets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01641194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = \"../data/trainSet/trainSet.txt\"\n",
    "test_data = \"../data/testSet/testSet.txt\"\n",
    "\n",
    "df_train = pd.read_csv(train_data)\n",
    "df_train_length_idx = len(df_train)\n",
    "df_test = pd.read_csv(test_data)\n",
    "\n",
    "# Merge the two datasets\n",
    "train_test = [df_train, df_test]\n",
    "df_train_test = pd.concat(train_test)\n",
    "# df_train_test['Prognosis'].loc[df_train_test['Prognosis'] == '<undefined>'] = np.nan\n",
    "df_train_test.loc[df_train_test['Prognosis'] == '<undefined>', 'Prognosis'] = np.nan\n",
    "\n",
    "# all variables in the dataset (incl. outcome)\n",
    "variables = list(df_train_test.columns[3:])\n",
    "# which variables to use for catboost (only numerical ones!)\n",
    "variables_for_regression = ['WBC', 'Temp_C', 'CRP', 'Fibrinogen', 'LDH', 'Ddimer', 'Ox_percentage', 'PaO2', 'SaO2', 'pH', 'Age']\n",
    "variables_for_classification = [ 'RespiratoryFailure', 'Sex', 'CardiovascularDisease', 'DifficultyInBreathing', 'Cough']\n",
    "\n",
    "# Set up catboost for each variable separately\n",
    "# Here we don't update and always use the original dataset with missing values in all variables\n",
    "\n",
    "# Generate new dataframe for imputed values (for this we copy the original test_trai datetset and store the index of the imputed ones and fill them in)\n",
    "df_imputed_train_test = df_train_test.copy()\n",
    "del df_imputed_train_test['Prognosis']\n",
    "\n",
    "# Prognosis should be Boolian\n",
    "df_imputed_train_test['Prognosis']= np.nan\n",
    "df_imputed_train_test.loc[df_train_test['Prognosis']=='MILD', 'Prognosis'] = 0\n",
    "df_imputed_train_test.loc[df_train_test['Prognosis']=='SEVERE', 'Prognosis'] = 1\n",
    "\n",
    "\n",
    "df_train_test['Prognosis'] = df_imputed_train_test['Prognosis']\n",
    "\n",
    "if not os.path.isfile(\"imputed_test.csv\"):\n",
    "    for catboost_variable in variables_for_regression:\n",
    "        print(catboost_variable)\n",
    "        catboost_features = variables.copy()\n",
    "        catboost_features.remove(catboost_variable)\n",
    "        catboost_df_train_test = df_train_test[df_train_test[catboost_variable].notna()]\n",
    "        X_train = catboost_df_train_test[catboost_features]\n",
    "        y_train = catboost_df_train_test[catboost_variable]\n",
    "\n",
    "        index_missing = df_train_test[catboost_variable].isna()\n",
    "        catboost_df_test = df_train_test[df_train_test[catboost_variable].isna()]\n",
    "        X_test = catboost_df_test[catboost_features]\n",
    "        y_test = catboost_df_test[catboost_variable]\n",
    "\n",
    "        # fit the model \n",
    "        model = CatBoostRegressor(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "\n",
    "        # make a prediction\n",
    "        yhat = model.predict(X_test.values)\n",
    "        df_imputed_train_test.loc[df_imputed_train_test[catboost_variable].isna(), catboost_variable] = yhat.T[0].copy()\n",
    "\n",
    "\n",
    "    for catboost_variable in variables_for_classification:\n",
    "        print(catboost_variable)\n",
    "        catboost_features = variables.copy()\n",
    "        catboost_features.remove(catboost_variable)\n",
    "        catboost_df_train_test = df_train_test[df_train_test[catboost_variable].notna()]\n",
    "        X_train = catboost_df_train_test[catboost_features]\n",
    "        y_train = catboost_df_train_test[catboost_variable]\n",
    "\n",
    "        index_missing = df_train_test[catboost_variable].isna()\n",
    "        catboost_df_test = df_train_test[df_train_test[catboost_variable].isna()]\n",
    "        X_test = catboost_df_test[catboost_features]\n",
    "        y_test = catboost_df_test[catboost_variable]\n",
    "\n",
    "        # fit the model \n",
    "        model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "\n",
    "        # make a prediction\n",
    "        yhat = model.predict(X_test.values)\n",
    "        df_imputed_train_test.loc[df_imputed_train_test[catboost_variable].isna(), catboost_variable] = yhat.T[0].copy()\n",
    "\n",
    "    # separate the two datasets:\n",
    "    df_imputed_train = df_imputed_train_test.iloc[0:df_train_length_idx, : ]\n",
    "    df_imputed_test = df_imputed_train_test.iloc[df_train_length_idx:, :]\n",
    "\n",
    "    df_imputed_train.to_csv(\"imputed_train.csv\")\n",
    "    df_imputed_test.to_csv(\"imputed_test.csv\")\n",
    "\n",
    "else:\n",
    "    df_imputed_train = pd.read_csv(\"imputed_train.csv\")\n",
    "    df_imputed_test = pd.read_csv(\"imputed_test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f94566",
   "metadata": {},
   "source": [
    "# Train tabular models\n",
    "### train catboost not imputed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c962e0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1ec98265940>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_for_rf = df_train.copy()\n",
    "df_for_rf[\"Prognosis\"] = df_for_rf[\"Prognosis\"].astype(\"category\").cat.codes\n",
    "df_for_rf = df_for_rf[variables]\n",
    "\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.8\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_for_rf[catboost_features].iloc[0:sep_index]\n",
    "y_train = df_for_rf[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "\n",
    "X_test = df_for_rf[catboost_features].iloc[sep_index:]\n",
    "y_test = df_for_rf[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit the model \n",
    "cat_boost_raw_model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "cat_boost_raw_model.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734720b",
   "metadata": {},
   "source": [
    "### xgboost not imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d25816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:24:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions from xgboost not imputed:\n",
    "df_for_xg = df_train.copy()\n",
    "df_for_xg[\"Prognosis\"] = df_for_xg[\"Prognosis\"].astype(\"category\").cat.codes\n",
    "df_for_xg = df_for_xg[variables]\n",
    "\n",
    "xgboost_features = variables.copy()\n",
    "xgboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.8\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_for_xg[xgboost_features].iloc[0:sep_index]\n",
    "y_train = df_for_xg[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "\n",
    "X_test = df_for_xg[xgboost_features].iloc[sep_index:]\n",
    "y_test = df_for_xg[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit model on training data\n",
    "model_xboost = XGBClassifier()\n",
    "model_xboost.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac6ec3",
   "metadata": {},
   "source": [
    "### lgblight not imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f52643f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get predictions from LightGBM not imputed:\n",
    "LGBM_features = variables.copy()\n",
    "LGBM_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.8\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_for_xg[LGBM_features].iloc[0:sep_index]\n",
    "y_train = df_for_xg[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "\n",
    "X_test = df_for_xg[LGBM_features].iloc[sep_index:]\n",
    "y_test = df_for_xg[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit model on training data\n",
    "lgb_model_raw = LGBMClassifier()\n",
    "lgb_model_raw.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a70351",
   "metadata": {},
   "source": [
    "### train catboost imputed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c4ba12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1ec98265c10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.8\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_imputed_train[catboost_features].iloc[0:sep_index]\n",
    "y_train = df_imputed_train[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "X_test = df_imputed_train[catboost_features].iloc[sep_index:]\n",
    "y_test = df_imputed_train[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit the model\n",
    "cat_boost_imputed_model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "cat_boost_imputed_model.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c2d8a",
   "metadata": {},
   "source": [
    "### train imputed random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1836f0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.values)\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "rf_regressor.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363ddb5",
   "metadata": {},
   "source": [
    "### xgboost imputed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f9905d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:25:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model on training data\n",
    "model_xboost_imputed = XGBClassifier()\n",
    "model_xboost_imputed.fit(X_train.values, y_train.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c78ec",
   "metadata": {},
   "source": [
    "# Get predictions to train final catboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4dc8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from catboost raw:\n",
    "df_for_rf = df_train.copy()\n",
    "df_for_rf[\"Prognosis\"] = df_for_rf[\"Prognosis\"].astype(\"category\").cat.codes\n",
    "df_for_rf = df_for_rf[variables]\n",
    "\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.6\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_test = df_for_rf[catboost_features].iloc[sep_index:]\n",
    "y_test = df_for_rf[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "yhat = cat_boost_raw_model.predict(X_test.values)\n",
    "\n",
    "catboost_results = dict(predictions_cb_raw=list(yhat), images=image_names.to_list())\n",
    "catboost_results_df = pd.DataFrame(catboost_results)\n",
    "\n",
    "# from xgboost raw:\n",
    "X_test = df_for_xg[xgboost_features].iloc[sep_index:]\n",
    "y_test = df_for_xg[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "y_pred = model_xboost.predict(X_test.values)\n",
    "xgboost_results = dict(predictions_xg=list(y_pred), images=image_names.to_list())\n",
    "xgboost_results_df = pd.DataFrame(xgboost_results)\n",
    "\n",
    "# from lgblight raw:\n",
    "y_pred = lgb_model_raw.predict(X_test.values)\n",
    "LGBM_results = dict(predictions_LGBM=list(y_pred), images=image_names.to_list())\n",
    "LGBM_results_df = pd.DataFrame(LGBM_results)\n",
    "\n",
    "# from catbtoost imputed:\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "X_test = df_imputed_train[catboost_features].iloc[sep_index:]\n",
    "y_test = df_imputed_train[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "yhat = cat_boost_imputed_model.predict(X_test.values)\n",
    "\n",
    "catboost_imputed_results = dict(predictions_cb_imputed=list(yhat), images=image_names.to_list(), targets=y_test.values)\n",
    "catboost_imputed_results_df = pd.DataFrame(catboost_imputed_results)\n",
    "\n",
    "# from random forest:\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "rf_results = dict(predictions_rf=list(y_pred), images=image_names.to_list())\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "\n",
    "# from xgboost imputed:\n",
    "y_pred = model_xboost_imputed.predict(X_test.values)\n",
    "xgboost_imputed_results = dict(predictions_xg_imputed=list(y_pred), images=image_names.to_list())\n",
    "xgboost_imputed_results_df = pd.DataFrame(xgboost_imputed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f92974e",
   "metadata": {},
   "source": [
    "# Stack predicions and train final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f6b68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_df = rf_results_df.set_index(\"images\")\n",
    "catboost_imputed_results_df = catboost_imputed_results_df.set_index(\"images\")\n",
    "catboost_results_df = catboost_results_df.set_index(\"images\")\n",
    "resnet_df = resnet_df.set_index(\"images\")\n",
    "xgboost_imputed_results_df = xgboost_imputed_results_df.set_index(\"images\")\n",
    "xgboost_results_df = xgboost_results_df.set_index(\"images\")\n",
    "LGBM_results_df = LGBM_results_df.set_index(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6b970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.concat([rf_results_df, catboost_imputed_results_df, catboost_results_df, resnet_df, xgboost_imputed_results_df, xgboost_results_df, LGBM_results_df], axis=1)\n",
    "X_train = all_predictions.drop(columns=\"targets\").values[:-10]\n",
    "X_test = all_predictions.drop(columns=\"targets\").values[-10:]\n",
    "y_train = all_predictions[\"targets\"].values[:-10]\n",
    "y_test = all_predictions[\"targets\"].values[-10:]\n",
    "\n",
    "# final stacking model:\n",
    "final_model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "final_model.fit(X_train, y_train)\n",
    "yhat = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a75f441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25c2286",
   "metadata": {},
   "source": [
    "# Get final predictions on testset for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfce59a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\Tobias\\Anaconda3\\envs\\pytorch_local\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from resnet:\n",
    "dl = HidaDataLoader(num_workers=0, batch_size=8, data_path=\"../data\", transform=transform)\n",
    "dl.setup(stage=\"test\")\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "test_dataloader = dl.test_dataloader()\n",
    "\n",
    "counter = 0\n",
    "tmpfile = \"tmpfile2.csv\"\n",
    "if os.path.isfile(tmpfile):\n",
    "    resnet_df = pd.read_csv(tmpfile)\n",
    "else:\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        model_input, label, (label_name, image_name) = batch\n",
    "        outputs_single = sc.expit(ort_sess.run([output_name], {input_name: model_input.cpu().numpy()})[0]).T[0]\n",
    "        predictions += list(outputs_single)\n",
    "        targets += list(image_name)\n",
    "\n",
    "    resnset_results = dict(predictions_resnet=predictions, images=targets)\n",
    "    resnet_df = pd.DataFrame(resnset_results)\n",
    "    resnet_df[\"images\"] = resnet_df[\"images\"].apply(lambda x: x.split(\".\")[0])\n",
    "    resnet_df.to_csv(tmpfile)\n",
    "\n",
    "# from catboost raw:\n",
    "df_for_rf = df_test.copy()\n",
    "df_for_rf[\"Prognosis\"] = df_for_rf[\"Prognosis\"].astype(\"category\").cat.codes\n",
    "df_for_rf = df_for_rf[variables]\n",
    "\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "X_test = df_for_rf[catboost_features]\n",
    "y_test = df_for_rf[\"Prognosis\"]\n",
    "image_names = df_test[\"PatientID\"]\n",
    "\n",
    "yhat = cat_boost_raw_model.predict(X_test.values)\n",
    "\n",
    "catboost_results = dict(predictions_cb_raw=list(yhat), images=image_names.to_list())\n",
    "catboost_results_df = pd.DataFrame(catboost_results)\n",
    "\n",
    "# from lgblight raw:\n",
    "y_pred = lgb_model_raw.predict(X_test.values)\n",
    "LGBM_results = dict(predictions_LGBM=list(y_pred), images=image_names.to_list(), targets=y_test.values)\n",
    "LGBM_results_df = pd.DataFrame(LGBM_results)\n",
    "\n",
    "\n",
    "# from xgboost raw:\n",
    "y_pred = model_xboost.predict(X_test.values)\n",
    "xgboost_results = dict(predictions_xg=list(y_pred), images=image_names.to_list())\n",
    "xgboost_results_df = pd.DataFrame(xgboost_results)\n",
    "\n",
    "# from catbtoost imputed:\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "X_test = df_imputed_test[catboost_features]\n",
    "y_test = df_imputed_test[\"Prognosis\"]\n",
    "image_names = df_test[\"PatientID\"]\n",
    "\n",
    "yhat = cat_boost_imputed_model.predict(X_test.values)\n",
    "\n",
    "catboost_imputed_results = dict(predictions_cb_imputed=list(yhat), images=image_names.to_list(), targets=y_test.values)\n",
    "catboost_imputed_results_df = pd.DataFrame(catboost_imputed_results)\n",
    "\n",
    "# from random forest:\n",
    "X_test_scaled = scaler.transform(X_test.values)\n",
    "y_pred = rf_regressor.predict(X_test_scaled)\n",
    "\n",
    "rf_results = dict(predictions_rf=list(y_pred), images=image_names.to_list())\n",
    "rf_results_df = pd.DataFrame(rf_results)\n",
    "\n",
    "# from xgboost imputed:\n",
    "y_pred = model_xboost_imputed.predict(X_test.values)\n",
    "xgboost_imputed_results = dict(predictions_xg_imputed=list(y_pred), images=image_names.to_list())\n",
    "xgboost_imputed_results_df = pd.DataFrame(xgboost_imputed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66365584",
   "metadata": {},
   "source": [
    "### stack final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ae80017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack predicions:\n",
    "rf_results_df = rf_results_df.set_index(\"images\")\n",
    "catboost_imputed_results_df = catboost_imputed_results_df.set_index(\"images\")\n",
    "catboost_results_df = catboost_results_df.set_index(\"images\")\n",
    "resnet_df = resnet_df.set_index(\"images\")\n",
    "xgboost_imputed_results_df = xgboost_imputed_results_df.set_index(\"images\")\n",
    "xgboost_results_df = xgboost_results_df.set_index(\"images\")\n",
    "LGBM_results_df = LGBM_results_df.set_index(\"images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee4799cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make final prediction:\n",
    "all_predictions = pd.concat([rf_results_df, catboost_imputed_results_df, catboost_results_df, resnet_df, xgboost_imputed_results_df, xgboost_results_df, LGBM_results_df], axis=1)\n",
    "X_test = all_predictions.drop(columns=\"targets\").values\n",
    "yhat = final_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "207ce982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       1.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "186f4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(test_data)\n",
    "\n",
    "for column in df_imputed_test.columns:\n",
    "    if column in df_out.columns:\n",
    "        df_out[column] = df_imputed_test[column]\n",
    "        \n",
    "df_out[\"Prognosis\"] = yhat\n",
    "df_out.to_csv(\"final_submission_lightning_fast_learner.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "40335de6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>ImageFile</th>\n",
       "      <th>Hospital</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Temp_C</th>\n",
       "      <th>Cough</th>\n",
       "      <th>DifficultyInBreathing</th>\n",
       "      <th>WBC</th>\n",
       "      <th>CRP</th>\n",
       "      <th>Fibrinogen</th>\n",
       "      <th>LDH</th>\n",
       "      <th>Ddimer</th>\n",
       "      <th>Ox_percentage</th>\n",
       "      <th>PaO2</th>\n",
       "      <th>SaO2</th>\n",
       "      <th>pH</th>\n",
       "      <th>CardiovascularDisease</th>\n",
       "      <th>RespiratoryFailure</th>\n",
       "      <th>Prognosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P_102</td>\n",
       "      <td>P_102.png</td>\n",
       "      <td>A</td>\n",
       "      <td>81.920072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.30000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.370000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>63.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>7.46000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P_117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.76346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.170000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>7.47873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_16</td>\n",
       "      <td>P_16.png</td>\n",
       "      <td>A</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.814336</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>65.0</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>7.47000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_118</td>\n",
       "      <td>P_118.png</td>\n",
       "      <td>A</td>\n",
       "      <td>81.920072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.76346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.720000</td>\n",
       "      <td>13.720000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>53.0</td>\n",
       "      <td>94.728545</td>\n",
       "      <td>7.60000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_114</td>\n",
       "      <td>P_114.png</td>\n",
       "      <td>A</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.76346</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.814336</td>\n",
       "      <td>22.509886</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>90.0</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>7.56000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>P_88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.70000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.140000</td>\n",
       "      <td>22.509886</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>89.0</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>7.56000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>P_92</td>\n",
       "      <td>P_92.png</td>\n",
       "      <td>A</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.40000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.170000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>83.0</td>\n",
       "      <td>94.728545</td>\n",
       "      <td>7.42000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>P_86</td>\n",
       "      <td>P_86.png</td>\n",
       "      <td>A</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.50000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.100000</td>\n",
       "      <td>20.760000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>7.43000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>P_9</td>\n",
       "      <td>P_9.png</td>\n",
       "      <td>A</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.610000</td>\n",
       "      <td>9.520000</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>552.000000</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>41.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>7.44000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>P_90</td>\n",
       "      <td>P_90.png</td>\n",
       "      <td>A</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.20000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.814336</td>\n",
       "      <td>22.509886</td>\n",
       "      <td>621.944903</td>\n",
       "      <td>297.317793</td>\n",
       "      <td>4847.134382</td>\n",
       "      <td>96.600503</td>\n",
       "      <td>71.0</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>7.49000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PatientID  ImageFile Hospital        Age  Sex    Temp_C  Cough  \\\n",
       "0       P_102  P_102.png        A  81.920072  1.0  37.30000    1.0   \n",
       "1       P_117        NaN        A  39.000000  1.0  37.76346    1.0   \n",
       "2        P_16   P_16.png        A  44.000000  0.0  38.00000    1.0   \n",
       "3       P_118  P_118.png        A  81.920072  0.0  37.76346    1.0   \n",
       "4       P_114  P_114.png        A  51.000000  1.0  37.76346    0.0   \n",
       "..        ...        ...      ...        ...  ...       ...    ...   \n",
       "115      P_88        NaN        A  58.000000  0.0  36.70000    1.0   \n",
       "116      P_92   P_92.png        A  48.000000  0.0  37.40000    1.0   \n",
       "117      P_86   P_86.png        A  87.000000  0.0  37.50000    1.0   \n",
       "118       P_9    P_9.png        A  70.000000  1.0  38.00000    0.0   \n",
       "119      P_90   P_90.png        A  72.000000  0.0  37.20000    0.0   \n",
       "\n",
       "     DifficultyInBreathing        WBC        CRP  Fibrinogen         LDH  \\\n",
       "0                      0.0   5.370000   3.960000  621.944903  297.317793   \n",
       "1                      0.0  10.170000   0.750000  621.944903  297.317793   \n",
       "2                      0.0   5.814336   5.200000  621.944903  297.317793   \n",
       "3                      0.0  17.720000  13.720000  621.944903  297.317793   \n",
       "4                      1.0   5.814336  22.509886  621.944903  297.317793   \n",
       "..                     ...        ...        ...         ...         ...   \n",
       "115                    0.0   5.140000  22.509886  621.944903  297.317793   \n",
       "116                    0.0   5.170000   2.600000  621.944903  178.000000   \n",
       "117                    0.0   8.100000  20.760000  621.944903  297.317793   \n",
       "118                    1.0   3.610000   9.520000  621.944903  297.317793   \n",
       "119                    0.0   5.814336  22.509886  621.944903  297.317793   \n",
       "\n",
       "          Ddimer  Ox_percentage  PaO2       SaO2       pH  \\\n",
       "0    4847.134382      93.000000  63.0  93.000000  7.46000   \n",
       "1    4847.134382      95.000000  77.0  97.000000  7.47873   \n",
       "2    4847.134382      96.600503  65.0  94.000000  7.47000   \n",
       "3    4847.134382      96.600503  53.0  94.728545  7.60000   \n",
       "4    4847.134382      96.600503  90.0  98.000000  7.56000   \n",
       "..           ...            ...   ...        ...      ...   \n",
       "115  4847.134382      96.600503  89.0  99.000000  7.56000   \n",
       "116  4847.134382      88.000000  83.0  94.728545  7.42000   \n",
       "117  4847.134382      89.000000  60.0  93.000000  7.43000   \n",
       "118   552.000000      96.600503  41.0  77.000000  7.44000   \n",
       "119  4847.134382      96.600503  71.0  98.000000  7.49000   \n",
       "\n",
       "     CardiovascularDisease  RespiratoryFailure  Prognosis  \n",
       "0                      0.0                 0.0        0.0  \n",
       "1                      1.0                 0.0        0.0  \n",
       "2                      0.0                 0.0        1.0  \n",
       "3                      0.0                 0.0        1.0  \n",
       "4                      0.0                 0.0        1.0  \n",
       "..                     ...                 ...        ...  \n",
       "115                    1.0                 0.0        0.0  \n",
       "116                    1.0                 0.0        1.0  \n",
       "117                    1.0                 0.0        1.0  \n",
       "118                    1.0                 0.0        1.0  \n",
       "119                    1.0                 0.0        1.0  \n",
       "\n",
       "[120 rows x 20 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f068041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
