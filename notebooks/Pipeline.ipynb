{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd626b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "from argparse import ArgumentParser\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from src.utils.DataLoader import HidaDataLoader\n",
    "import pandas as pd\n",
    "import scipy.special as sc\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1eaac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.CenterCrop(size=(900,900)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "dl = HidaDataLoader(num_workers=0, batch_size=8, data_path=\"../data\", transform=transform)\n",
    "dl.setup()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5c9e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                   | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_678.png', 'P_673.png', 'P_669.png', 'P_675.png', 'P_640.png', 'P_694.png', 'P_692.png', 'P_807.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|███████████████▌                                                                                                                                                           | 1/11 [00:08<01:25,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_810.png', 'P_811.png', 'P_796.png', 'P_795.png', 'P_756.png', 'P_824.png', 'P_768.png', 'P_814.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|███████████████████████████████                                                                                                                                            | 2/11 [00:17<01:20,  8.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_783.png', 'P_819.png', 'P_833.png', 'P_843.png', 'P_828.png', 'P_791.png', 'P_803.png', 'P_801.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██████████████████████████████████████████████▋                                                                                                                            | 3/11 [00:27<01:13,  9.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_825.png', 'P_835.png', 'P_829.png', 'P_841.png', 'P_832.png', 'P_1_7.png', 'P_1_1.png', 'P_844.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|██████████████████████████████████████████████████████████████▏                                                                                                            | 4/11 [00:36<01:04,  9.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_92.png', 'P_1_57.png', 'P_1_56.png', 'P_1_103.png', 'P_1_142.png', 'P_1_87.png', 'P_1_101.png', 'P_1_59.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████████████████████████████████████████████████████████████▋                                                                                             | 5/11 [00:45<00:55,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_111.png', 'P_1_102.png', 'P_1_154.png', 'P_1_82.png', 'P_1_61.png', 'P_1_73.png', 'P_1_69.png', 'P_1_44.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                                             | 6/11 [00:55<00:46,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_54.png', 'P_1_51.png', 'P_1_23.png', 'P_1_76.png', 'P_1_31.png', 'P_1_55.png', 'P_1_20.png', 'P_1_17.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                              | 7/11 [01:04<00:37,  9.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_37.png', 'P_1_34.png', 'P_1_16.png', 'P_1_22.png', 'P_1_25.png', 'P_1_42.png', 'P_1_65.png', 'P_1_100.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 8/11 [01:14<00:28,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_107.png', 'P_1_79.png', 'P_1_60.png', 'P_1_123.png', 'P_1_134.png', 'P_1_156.png', 'P_1_47.png', 'P_1_118.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 9/11 [01:23<00:18,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_53.png', 'P_1_80.png', 'P_1_85.png', 'P_1_21.png', 'P_1_163.png', 'P_1_128.png', 'P_1_77.png', 'P_1_110.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 10/11 [01:32<00:09,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('P_1_126.png', 'P_1_18.png', 'P_1_12.png', 'P_1_8.png', 'P_1_10.png', 'P_1_26.png', 'P_1_146.png')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [01:40<00:00,  9.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# get predictions from ResNet image classifier1:\n",
    "\n",
    "ONNX_FILE = \"C:/Users/Tobias/PycharmProjects/HIDA_LFL/logs/checkpoints/HIDA/model_193.onnx\"\n",
    "options = ort.SessionOptions()\n",
    "options.inter_op_num_threads = 12\n",
    "options.intra_op_num_threads = 12\n",
    "\n",
    "ort_sess = ort.InferenceSession(ONNX_FILE, sess_options=options)\n",
    "ort_sess.get_inputs()[0].name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "valid_dataloader = dl.val_dataloader()\n",
    "\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "output_name = ort_sess.get_outputs()[0].name\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "counter = 0\n",
    "for batch in tqdm(valid_dataloader):\n",
    "    model_input, label, (label_name, image_name) = batch\n",
    "\n",
    "    outputs_single = sc.expit(ort_sess.run([output_name], {input_name: model_input.cpu().numpy()})[0]).T[0]\n",
    "    predictions += list(outputs_single)\n",
    "    targets += list(image_name)\n",
    "    counter += 1\n",
    "\n",
    "resnset_results = dict(predictions_resnet=predictions, images=targets)\n",
    "resnet_df = pd.DataFrame(resnset_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d9c51a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from VIT image classifier:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db031aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WBC\n",
      "Temp_C\n",
      "CRP\n",
      "Fibrinogen\n",
      "LDH\n",
      "Ddimer\n",
      "Ox_percentage\n",
      "PaO2\n",
      "SaO2\n",
      "pH\n",
      "Age\n",
      "RespiratoryFailure\n",
      "Sex\n",
      "CardiovascularDisease\n",
      "DifficultyInBreathing\n",
      "Cough\n"
     ]
    }
   ],
   "source": [
    "# impute datasets:\n",
    "\n",
    "train_data = \"../data/trainSet/trainSet.txt\"\n",
    "test_data = \"../data/testSet/testSet.txt\"\n",
    "\n",
    "df_train = pd.read_csv(train_data)\n",
    "df_train_length_idx = len(df_train)\n",
    "df_test = pd.read_csv(test_data)\n",
    "\n",
    "# Merge the two datasets\n",
    "train_test = [df_train, df_test]\n",
    "df_train_test = pd.concat(train_test)\n",
    "# df_train_test['Prognosis'].loc[df_train_test['Prognosis'] == '<undefined>'] = np.nan\n",
    "df_train_test.loc[df_train_test['Prognosis'] == '<undefined>', 'Prognosis'] = np.nan\n",
    "\n",
    "# all variables in the dataset (incl. outcome)\n",
    "variables = list(df_train_test.columns[3:])\n",
    "# which variables to use for catboost (only numerical ones!)\n",
    "variables_for_regression = ['WBC', 'Temp_C', 'CRP', 'Fibrinogen', 'LDH', 'Ddimer', 'Ox_percentage', 'PaO2', 'SaO2', 'pH', 'Age']\n",
    "variables_for_classification = [ 'RespiratoryFailure', 'Sex', 'CardiovascularDisease', 'DifficultyInBreathing', 'Cough']\n",
    "\n",
    "# Set up catboost for each variable separately\n",
    "# Here we don't update and always use the original dataset with missing values in all variables\n",
    "\n",
    "# Generate new dataframe for imputed values (for this we copy the original test_trai datetset and store the index of the imputed ones and fill them in)\n",
    "df_imputed_train_test = df_train_test.copy()\n",
    "del df_imputed_train_test['Prognosis']\n",
    "\n",
    "# Prognosis should be Boolian\n",
    "df_imputed_train_test['Prognosis']= np.nan\n",
    "df_imputed_train_test.loc[df_train_test['Prognosis']=='MILD', 'Prognosis'] = 0\n",
    "df_imputed_train_test.loc[df_train_test['Prognosis']=='SEVERE', 'Prognosis'] = 1\n",
    "\n",
    "\n",
    "df_train_test['Prognosis'] = df_imputed_train_test['Prognosis']\n",
    "\n",
    "if not os.path.isfile(\"imputed_test.csv\"):\n",
    "    for catboost_variable in variables_for_regression:\n",
    "        print(catboost_variable)\n",
    "        catboost_features = variables.copy()\n",
    "        catboost_features.remove(catboost_variable)\n",
    "        catboost_df_train_test = df_train_test[df_train_test[catboost_variable].notna()]\n",
    "        X_train = catboost_df_train_test[catboost_features]\n",
    "        y_train = catboost_df_train_test[catboost_variable]\n",
    "\n",
    "        index_missing = df_train_test[catboost_variable].isna()\n",
    "        catboost_df_test = df_train_test[df_train_test[catboost_variable].isna()]\n",
    "        X_test = catboost_df_test[catboost_features]\n",
    "        y_test = catboost_df_test[catboost_variable]\n",
    "\n",
    "        # fit the model \n",
    "        model = CatBoostRegressor(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "\n",
    "        # make a prediction\n",
    "        yhat = model.predict(X_test.values)\n",
    "        df_imputed_train_test.loc[df_imputed_train_test[catboost_variable].isna(), catboost_variable] = yhat.T[0].copy()\n",
    "\n",
    "\n",
    "    for catboost_variable in variables_for_classification:\n",
    "        print(catboost_variable)\n",
    "        catboost_features = variables.copy()\n",
    "        catboost_features.remove(catboost_variable)\n",
    "        catboost_df_train_test = df_train_test[df_train_test[catboost_variable].notna()]\n",
    "        X_train = catboost_df_train_test[catboost_features]\n",
    "        y_train = catboost_df_train_test[catboost_variable]\n",
    "\n",
    "        index_missing = df_train_test[catboost_variable].isna()\n",
    "        catboost_df_test = df_train_test[df_train_test[catboost_variable].isna()]\n",
    "        X_test = catboost_df_test[catboost_features]\n",
    "        y_test = catboost_df_test[catboost_variable]\n",
    "\n",
    "        # fit the model \n",
    "        model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "        model.fit(X_train.values, y_train.values)\n",
    "\n",
    "        # make a prediction\n",
    "        yhat = model.predict(X_test.values)\n",
    "        df_imputed_train_test.loc[df_imputed_train_test[catboost_variable].isna(), catboost_variable] = yhat.T[0].copy()\n",
    "\n",
    "    # separate the two datasets:\n",
    "    df_imputed_train = df_imputed_train_test.iloc[0:df_train_length_idx, : ]\n",
    "    df_imputed_test = df_imputed_train_test.iloc[df_train_length_idx:, :]\n",
    "\n",
    "    df_imputed_train.to_csv(\"imputed_train.csv\")\n",
    "    df_imputed_test.to_csv(\"imputed_test.csv\")\n",
    "\n",
    "else:\n",
    "    df_imputed_train = pd.read_csv(\"imputed_train.csv\")\n",
    "    df_imputed_test = pd.read_csv(\"imputed_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b27a84e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from catboost not imputed:\n",
    "\n",
    "df_for_rf = df_train.copy()\n",
    "df_for_rf[\"Prognosis\"] = df_for_rf[\"Prognosis\"].astype(\"category\").cat.codes\n",
    "df_for_rf = df_for_rf[variables]\n",
    "\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.9\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_for_rf[catboost_features].iloc[0:sep_index]\n",
    "y_train = df_for_rf[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "\n",
    "X_test = df_for_rf[catboost_features].iloc[sep_index:]\n",
    "y_test = df_for_rf[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit the model \n",
    "model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(X_test.values)\n",
    "\n",
    "catboost_results = dict(predictions_cb_raw=list(yhat), images=image_names.to_list())\n",
    "catboost_results_df = pd.DataFrame(catboost_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75fdb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from catboost imputed:\n",
    "catboost_features = variables.copy()\n",
    "catboost_features.remove(\"Prognosis\")\n",
    "\n",
    "train_valid_split = 0.9\n",
    "sep_index = int(863 * train_valid_split)\n",
    "\n",
    "X_train = df_imputed_train[catboost_features].iloc[0:sep_index]\n",
    "y_train = df_imputed_train[\"Prognosis\"].iloc[0:sep_index]\n",
    "\n",
    "X_test = df_imputed_train[catboost_features].iloc[sep_index:]\n",
    "y_test = df_imputed_train[\"Prognosis\"].iloc[sep_index:]\n",
    "image_names = df_train[\"ImageFile\"].iloc[sep_index:]\n",
    "\n",
    "# fit the model \n",
    "model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# make a prediction\n",
    "yhat = model.predict(X_test.values)\n",
    "\n",
    "catboost_imputed_results = dict(predictions_cb_imputed=list(yhat), images=image_names.to_list(), targets=y_test.values)\n",
    "catboost_imputed_results_df = pd.DataFrame(catboost_imputed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "065edf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions from imputed random forest:\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train.values)\n",
    "X_test_scaled = sc.transform(X_test.values)\n",
    "\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "regressor.fit(X_train_scaled, y_train)\n",
    "y_pred = regressor.predict(X_test_scaled)\n",
    "\n",
    "rf_results = dict(predictions_rf=list(y_pred), images=image_names.to_list())\n",
    "rf_results_df = pd.DataFrame(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f139d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack predicions:\n",
    "rf_results_df = rf_results_df.set_index(\"images\")\n",
    "catboost_imputed_results_df = catboost_imputed_results_df.set_index(\"images\")\n",
    "catboost_results_df = catboost_results_df.set_index(\"images\")\n",
    "resnet_df = resnet_df.set_index(\"images\")\n",
    "\n",
    "all_predictions = pd.concat([rf_results_df, catboost_imputed_results_df,resnet_df, catboost_results_df], axis=1)\n",
    "X_train = all_predictions.drop(columns=\"targets\").values[:-10]\n",
    "X_test = all_predictions.drop(columns=\"targets\").values[-10:]\n",
    "y_train = all_predictions[\"targets\"].values[:-10]\n",
    "y_test = all_predictions[\"targets\"].values[-10:]\n",
    "\n",
    "\n",
    "# final stacking model:\n",
    "model = CatBoostClassifier(verbose=0, iterations=1000, task_type=\"GPU\", devices='0:1')\n",
    "model.fit(X_train, y_train)\n",
    "yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0efc5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "0.0 0.0\n",
      "0.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 0.0\n",
      "1.0 0.0\n",
      "0.0 0.0\n",
      "1.0 1.0\n",
      "0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "for x,y in zip(y_test, yhat):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10823c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
